State-of-the-Art Training Strategies for Probabilistic Forecasting of Rare, High-Impact EventsIntroductionThe accurate forecasting of time series data is a cornerstone of modern data-driven decision-making, with applications spanning finance, energy, and logistics.1 Within this domain, the prediction of rare, high-impact, extreme events represents a particularly challenging frontier with profound societal and economic implications.2 Hydrological time series, such as river flow and water levels, exemplify this challenge. These datasets are often characterized by long periods of quasi-stationary, predictable behavior, punctuated by infrequent but catastrophic flood events.4 While deep learning models have demonstrated exceptional performance in capturing average temporal dynamics, they frequently fail to accurately predict the magnitude and probability of these extreme events.6 This failure stems not from a lack of model capacity, but from fundamental limitations in standard training paradigms that are optimized for mean performance over a balanced data distribution.The core of the issue lies in the severe class imbalance inherent in these datasets, a phenomenon termed the "Curse of Rarity".2 Extreme events, by definition, constitute a vanishingly small fraction of the training data. Consequently, conventional loss functions, such as Mean Squared Error (MSE) or even the more sophisticated Continuous Ranked Probability Score (CRPS), are dominated by the gradients of the abundant "normal" samples.6 This biases the model towards underestimating the tail end of the distribution, treating the very events of greatest interest as noise or outliers to be ignored in favor of minimizing the average error.4 The result is a model that may be accurate on average but is unreliable when it matters most—in anticipating the most severe scenarios.6The selection of a sophisticated model architecture, such as a Transformer with Mixture-of-Experts (MoE) layers for temporal feature extraction and a Spliced Binned Pareto distribution head for modeling heavy-tailed phenomena, is a well-justified approach.8 Transformers are adept at capturing long-range dependencies, which are often crucial for understanding the precursors to extreme events, while a specialized heavy-tailed output distribution acknowledges the statistical nature of the problem from the outset.9 However, this advanced architecture cannot realize its full potential if the training process itself does not explicitly address the data imbalance. The performance gap on extreme events is therefore not an architectural flaw but a training data and optimization challenge.This report provides a systematic, state-of-the-art review of training strategies designed to rectify this imbalance and improve the probabilistic forecasting of rare, high-impact events. Assuming a fixed, high-capacity model architecture and a standard probabilistic loss function (CRPS), this review focuses exclusively on methodologies that manipulate the training process to compel the model to learn from the underrepresented, high-impact segments of the data. The subsequent sections are structured to provide a comprehensive and actionable overview of four key areas of intervention:Advanced Data Sampling Strategies: Moving beyond naive resampling to explore principled, dynamic methods for constructing mini-batches that give appropriate emphasis to rare events.Dynamic Loss Weighting and Gradient Modulation: Investigating techniques that directly modify the optimization objective or the gradients themselves to prioritize learning on extreme events.Self-Supervised and Contrastive Learning: Examining the use of self-supervised learning, particularly contrastive methods, as an auxiliary objective to improve the quality of learned representations and explicitly distinguish between "normal" and "extreme" regimes.Minor Architectural Considerations: Briefly covering small, non-fundamental architectural tweaks that enhance training stability when dealing with the high dynamic range of heavy-tailed data.By surveying the most recent and forward-looking research from late 2023 through early 2025, this report aims to equip practitioners and researchers with the tools necessary to build models that are not only accurate on average but also robust and reliable in the face of rare and catastrophic events.Advanced Data Sampling Strategies for Imbalanced Time SeriesThe most direct approach to addressing data imbalance is to alter the composition of the data seen by the model during training. While simple methods like randomly oversampling minority class instances or undersampling majority class instances are common in classification, they are often suboptimal for time series forecasting. Naive duplication of rare event windows can lead to severe overfitting, while discarding "boring" data can cause the model to lose crucial information about baseline dynamics and temporal continuity.11 State-of-the-art approaches therefore move beyond these heuristics to principled, dynamic methods for constructing mini-batches that give appropriate, and often adaptive, emphasis to the most informative samples. These methods treat each training window not as an equal participant, but as a sample with a specific "importance" or "hardness" that should dictate its frequency in the training process.From Static to Dynamic Batching: The Principle of Importance SamplingThe foundational theory for more advanced sampling techniques is importance sampling, a variance reduction technique from Monte Carlo methods.14 The core idea is to sample from a different, more convenient proposal distribution (q(x)) instead of the original data distribution (p(x)) and then correct for this change by weighting the samples. The expectation of a function f(x) under p(x) can be rewritten as:Ep​[f(X)]=∫f(x)p(x)dx=∫f(x)q(x)p(x)​q(x)dx=Eq​[f(X)q(X)p(X)​]In the context of training a neural network, this has a powerful implication. Instead of sampling uniformly from the dataset, one can preferentially sample from a proposal distribution q(x) that over-represents "important" regions—in this case, windows containing extreme events. The bias introduced by this non-uniform sampling is then corrected by weighting the loss contribution of each sample by its importance weight, w(x)=q(x)p(x)​.16In practice, this is often implemented not by directly weighting the loss, but by using the importance scores to build a discrete probability distribution over the entire training set. A weighted random sampler then draws mini-batches according to this distribution. Frameworks like PyTorch provide convenient tools for this, such as torch.utils.data.WeightedRandomSampler. This sampler takes a vector of weights, one for each sample in the dataset, and constructs batches where the probability of a sample being chosen is proportional to its weight.18 This transforms the problem of creating balanced batches into the problem of defining a robust and effective "hardness" or "importance" score for each training window.A Taxonomy of "Hardness" Scores for Prioritized SamplingThe effectiveness of any weighted sampling strategy hinges entirely on the definition of the sample weights. A "hard" or "important" sample is one that provides a rich learning signal, forcing the model to update its parameters in a meaningful way. The definition of hardness is not static; it is an evolving dialogue between the intrinsic properties of the data and the current state of the model's knowledge.19 A sample is "hard" relative to what the model already knows. An extreme flood event might be hard initially, but after sufficient training, a more subtle precursor pattern might become the "hardest" sample to learn. This dynamic nature suggests that the most effective strategies often evolve from static, data-driven scores to dynamic, model-based scores as training progresses. A comprehensive taxonomy of these scoring methods is presented below and summarized in Table 1.Data-Driven Scores (Static & Pre-computed)These scores are derived from the intrinsic properties of the data within a given time series window and are calculated before training begins. They are independent of the model's state and provide a fixed view of sample importance.Magnitude-based Scoring: This is the most direct and intuitive approach. Windows are assigned higher weights if their target values exceed a predefined absolute threshold or a high percentile (e.g., the 99th percentile) of the overall data distribution. For hydrological data, this directly prioritizes windows containing flood peaks.21Volatility-based Scoring: Extreme events are often preceded or accompanied by periods of high instability. Weights can be assigned based on statistical measures of volatility within a window, such as the standard deviation, variance, or more sophisticated measures of long-range dependence like the Hurst exponent.23 This allows the model to focus not just on the peak of the event but also on the transitional dynamics leading up to it.Rate-of-Change-based Scoring: This method scores windows based on the magnitude of the first or second derivative of the time series. It prioritizes periods of rapid change, which is particularly effective for capturing the onset of flash floods or sudden shifts in system behavior.25Information-Theoretic Scoring: More advanced methods can score windows based on their information content. For example, one could calculate the mutual information between the features in a window and the target variable. Windows with higher mutual information are more predictive and can be assigned higher weights.26Model-Based Scores (Dynamic & Iterative)These scores are derived from the model's feedback during the training process. They are dynamic, allowing the definition of "hardness" to adapt as the model learns and its understanding of the data evolves.Reconstruction Error: A powerful and increasingly popular technique involves pre-training a separate, unsupervised model—typically an autoencoder or a Variational Autoencoder (VAE)—exclusively on "normal" data (e.g., all data below the 95th percentile). During the main training loop, each window is passed through this pre-trained autoencoder. The reconstruction error is then used as the hardness score. Windows containing patterns that the autoencoder has not seen before (i.e., rare and extreme events) will produce a high reconstruction error and thus be assigned a high weight.28 This method effectively turns the problem of defining "extreme" into a well-defined anomaly detection task.Instantaneous Loss: The most direct form of model-based feedback is the loss itself. The loss value (e.g., CRPS) from the previous training step for a given sample can be used as its hardness score for the next sampling step. Samples that the model struggles with (i.e., produce a high loss) are, by definition, hard and should be sampled more frequently.20 This creates a tight feedback loop where the model focuses its attention on its own mistakes.Dynamic Instance Hardness (DIH): A significant drawback of using instantaneous loss is its high variance due to stochastic gradient descent and the non-smooth nature of deep learning loss landscapes.20 A more stable and robust metric is the Dynamic Instance Hardness (DIH), defined as the exponential moving average of a sample's instantaneous hardness (e.g., its loss) over the training history.20 A sample with a consistently high DIH is one that the model persistently struggles to learn or "forgets" easily. Research has shown that a sample's DIH converges quickly early in training, making it a reliable and computationally inexpensive predictor of long-term learning difficulty.32Prediction Instability: Hardness can also be framed as model uncertainty. For a given input window, one can measure the variance of predictions from an ensemble of models or by using Monte Carlo dropout. Windows that produce high-variance predictions are those where the model is uncertain, indicating they lie in a complex or under-sampled region of the input space. These uncertain samples can be prioritized for training.20Analogy to Reinforcement Learning: Prioritized Experience Replay (PER)The concept of using model-based hardness scores finds a strong theoretical and empirical parallel in the field of deep reinforcement learning (RL) with Prioritized Experience Replay (PER).34 In standard RL algorithms like DQN, past experiences (transitions) are stored in a replay buffer and sampled uniformly. PER improves upon this by assigning a priority to each transition based on the magnitude of its Temporal-Difference (TD) error—the difference between the predicted value and the observed reward plus discounted future value.36 Transitions with high TD error, representing moments of high surprise or learning potential, are replayed more frequently.This provides a powerful analogy for time series forecasting. The one-step-ahead prediction error (e.g., the CRPS loss for a given window) is directly analogous to the TD error. Just as PER accelerates learning in RL by focusing on high-error transitions, prioritizing high-loss time series windows can make the training process more efficient and effective, forcing the model to rapidly correct its most significant mistakes, which are often associated with rare events.34 To correct for the bias introduced by this prioritized sampling, PER uses importance-sampling weights to scale the gradient updates, a practice that can also be adopted in the forecasting context.35Table 1: A Comparative Taxonomy of Hardness Score Metrics for Prioritized SamplingMetric CategorySpecific MetricCore PrinciplePros for Hydrological ExtremesCons/ChallengesKey ReferencesData-Driven (Static)Target MagnitudeWeight samples based on the absolute value or percentile of the target variable.Simple, direct, and guaranteed to focus on the highest-magnitude flood peaks.Ignores precursor dynamics; may miss important low-magnitude events in smaller tributaries.21Volatility (Std. Dev., Hurst Exp.)Weight samples based on the statistical variance or long-range dependence within the window.Captures periods of instability that often precede or co-occur with extreme events.Can be sensitive to noise; may not distinguish between benign volatility and pre-event turmoil.23Rate of Change (Derivative)Weight samples based on the speed of change in the target variable.Excellent for prioritizing the onset of flash floods and rapid system state changes.Less effective for slow-onset events like droughts; sensitive to high-frequency noise.25Model-Based (Dynamic)Reconstruction ErrorWeight samples based on the reconstruction error from an autoencoder pre-trained on "normal" data.Principled anomaly detection; identifies patterns that are structurally different, not just high-magnitude.Requires training and maintaining a separate unsupervised model, adding complexity.28Instantaneous LossUse the loss from the previous training step as the weight for the current step.Direct feedback loop; model focuses on what it currently finds difficult.Can be very noisy and unstable due to SGD randomness and non-smooth loss landscapes.20Dynamic Instance Hardness (DIH)Use an exponential moving average of the loss over the training history.Smooths out the noise of instantaneous loss, providing a stable measure of long-term learning difficulty.Requires maintaining a historical record of losses for each sample, increasing memory overhead.20Prediction InstabilityWeight samples based on the variance of predictions from an ensemble or MC dropout.Identifies regions of high model uncertainty, which often correspond to under-sampled or complex dynamics.Computationally expensive, requiring multiple forward passes per sample to estimate uncertainty.20Curriculum and Self-Paced Learning for Extreme EventsWhile weighted random sampling adjusts the frequency of hard samples, curriculum learning (CL) and self-paced learning (SPL) adjust the timing.38 These strategies are motivated by the human learning process, where simple concepts are mastered before more complex ones are introduced.38 In the context of extreme events, this paradigm is fundamentally about managing model stability and plasticity. A complex Transformer model exposed to high-magnitude, high-loss extreme events from the very beginning of training can experience large, chaotic gradient updates. This can destabilize the learning process, pushing the model into a poor local minimum or causing it to "forget" the patterns of the much more common normal data.40Curriculum learning acts as an implicit learning rate scheduler and regularizer. By training first on "easy" data—the long stretches of quasi-stationary behavior—the model can establish a robust and stable representation of the system's baseline dynamics. Once this foundation is in place, the "hard" data—the rare, high-impact events—can be introduced. The parameter updates required to accommodate these events are then smaller and more targeted, akin to fine-tuning rather than a chaotic relearning process. This structured exposure ensures that the model learns to handle the tail of the distribution without sacrificing its performance on the body.Methodologies for Curriculum DesignThe design of the curriculum—the definition of "easy" and "hard" and the schedule for introducing them—is the central challenge.Difficulty-based Pacing: The most straightforward approach is to use the hardness scores defined in the previous section to create a curriculum. For instance, training can begin with samples in the lowest 50% of volatility scores. After a set number of epochs, the training set is expanded to include the lowest 75%, and finally the full dataset.38 This can be based on any hardness metric, such as sequence length or window size, where shorter sequences are considered easier.39Automated, Loss-Driven Approaches: Manually designing a curriculum can be brittle and require significant domain expertise. More advanced, state-of-the-art methods automate this process. A notable example is the Curricular and CyclicaL (CRUCIAL) loss framework.40 CRUCIAL is a model-agnostic loss wrapper that jointly learns the model parameters and the sample curriculum. It achieves this by dynamically modulating the loss amplitude of each sample based on the model's confidence, automatically up-weighting the contribution of hard samples (those with large loss) as training progresses. This effectively creates an "easy-to-hard" learning order without requiring a pre-defined schedule. Furthermore, CRUCIAL introduces a cyclical training set size, which has been shown to be more effective than a monotonically increasing size for dynamic time series data.40Gradient-based Curriculum Design: The most advanced frontier in curriculum learning involves using gradient information to guide sample selection. These methods aim to select a subset of samples at each step that maximizes the learning progress. One such approach is to select samples that are at the "learning frontier"—those that have a large current loss but are also learned quickly (i.e., have a large gradient norm or cause a significant change in the model's output).41 By focusing on these samples, the model can achieve the most efficient optimization trajectory. This is a computationally intensive but principled way to construct an optimal curriculum.41Dynamic Loss Weighting and Gradient ModulationWhile data sampling strategies alter the distribution of data fed to the model, a more direct approach is to modify the optimization objective itself. By dynamically weighting the loss contribution of each sample within a mini-batch or directly modulating the gradients, these techniques can force the optimizer to prioritize the learning signal from rare, high-impact events. This represents a more surgical intervention than sampling, as it reshapes the loss landscape that the model traverses during training, creating steeper penalties for errors on the events of interest.Instance-Level Loss Re-weighting for High-Impact EventsThe fundamental principle of instance-level re-weighting is to modify the standard batch loss, Lbatch​=B1​∑i=1B​Li​, to an explicitly weighted form, Lbatch​=∑wi​1​∑i=1B​wi​Li​, where B is the batch size, Li​ is the loss for the i-th sample, and wi​ is a dynamically computed weight reflecting its importance.11 The challenge lies in determining the optimal weighting scheme, wi​, to effectively balance the learning between normal and extreme events.State-of-the-Art Frameworks for Dynamic WeightingRecent research has moved beyond simple heuristic weighting (e.g., weighting by inverse class frequency) towards more adaptive and principled frameworks.Meta-Learning for Optimal Weights: A particularly powerful and sophisticated approach treats the loss weights, wi​, not as fixed heuristics but as learnable parameters.44 This is framed as a bilevel optimization problem. In the inner loop, the model's parameters (θ) are updated by taking a gradient step on the weighted training loss. In the outer loop, the loss weights (w) are updated by taking a gradient step to minimize the model's performance on a separate, clean, and unbiased validation set composed exclusively of extreme event samples. This process effectively "learns to re-weight" by discovering the weighting scheme that leads to the best generalization performance on the rare events of interest. The gradient for the weights is computed through the model's parameter update, allowing the validation performance to directly inform how the training samples should be prioritized.44Self-adaptive Extreme Penalized Loss (EPL): This framework proposes a novel, tripartite loss function that explicitly handles the asymmetric risks associated with predicting extreme events.6 Instead of a binary distinction between "normal" and "extreme," EPL divides the penalty into three cases based on the prediction error relative to an extreme event threshold, γ:Normal Events: Errors on normal samples are penalized using a standard MSE loss, preserving accuracy on the bulk of the data.Underestimated Extreme Events: Errors where the ground truth is extreme but the prediction is not (or is too low) are penalized heavily, often using an exponential function like e−x−1, to strongly discourage under-prediction.Overestimated Extreme Events: Errors where an extreme event is over-predicted are penalized more leniently, for example, with ex/λ−1. This relaxation acknowledges that overestimating risk is often less costly than underestimating it.A key advantage of this approach is its self-adaptive nature. The extreme event threshold, γ, and the over-prediction penalty strength, λ, are not treated as hyperparameters to be tuned manually but are determined adaptively from the training data's distribution and the prediction task's horizon length.6Principled Approaches from Extreme Value Theory (EVT)A fundamental issue with simply re-weighting a standard loss like MSE is that it still implicitly assumes the underlying error distribution is Gaussian, which is a poor match for heavy-tailed data. A more principled approach is to build the loss function itself from a heavy-tailed likelihood, a strategy informed by Extreme Value Theory (EVT).6Generalized Extreme Value Loss (GEVL): The GEVL framework directly addresses this mismatch.4 It proves that optimizing with a standard square loss is equivalent to performing kernel density estimation (KDE) with a light-tailed Gaussian kernel, which is inherently incapable of modeling a heavy-tailed ground-truth distribution. GEVL replaces this implicit Gaussian kernel with kernels derived from heavy-tailed distributions from EVT, such as the Gumbel, Weibull, or Fréchet distributions. This results in a new family of loss functions that are fundamentally aligned with the statistical properties of extreme events. For instance, the Fréchet GEVL is particularly suited for very heavy-tailed data and has been shown to reduce RMSE on extreme events by over 17-20% on average in tasks like stock price prediction.4 These loss functions do not just make the penalty for large errors steeper; they change the shape of the entire loss surface to make large errors more "expected" by the optimizer, leading to more stable and effective learning of tail behavior.Adapting Focal Loss for Heavy-Tailed Probabilistic RegressionFocal Loss was originally proposed for object detection to address the extreme imbalance between foreground (objects) and background classes.46 Its core mechanism is to down-weight the loss contribution from "easy," well-classified examples, thereby allowing the model to focus its training efforts on "hard," misclassified examples.30 This is achieved by adding a modulating factor, (1−pt​)γ, to the standard cross-entropy loss, where pt​ is the model's predicted probability for the ground-truth class and γ is a tunable focusing parameter.46Adapting this principle to a continuous regression task like time series forecasting is non-trivial because there is no direct equivalent of pt​.50 However, recent research has developed effective adaptations that are highly relevant to the problem of extreme event forecasting.Error-based Heuristics: A straightforward adaptation defines the modulating factor based on the magnitude of the regression error. For instance, an MAE-Focal Loss can be formulated where the loss for a sample is scaled by a factor related to its absolute error, such as (|y - y_hat| / sigma)^gamma, where sigma is a scaling factor.4 This effectively applies the "focusing" principle by up-weighting the contribution of samples with large prediction errors, which are often the extreme events.Generalized Focal Loss (GFL): A more principled and powerful approach is the Generalized Focal Loss (GFL).51 GFL was designed to allow the focal loss concept to be applied to continuous targets. It achieves this by discretizing the continuous target variable into a set of bins and reformulating the regression problem as a classification problem over these bins. The model predicts a probability distribution over the bins, and the final continuous value is calculated as the expected value of this distribution.53This GFL framework provides a direct and elegant bridge to the user's specified model architecture, which already employs a Spliced Binned Pareto distribution head. This head naturally discretizes the output space into bins. Instead of inventing a complex "Focal CRPS," one can apply the GFL directly to the logits or probabilities of the distribution's bins. The GFL would then operate in two parts:Quality Focal Loss (QFL): This component treats the problem as a regression of a continuous value (the probability of the correct bin) using a focal-style loss, effectively focusing on bins where the model is uncertain or wrong.51Distribution Focal Loss (DFL): This component encourages the predicted distribution over the bins to be sharp and concentrated around the true value, penalizing diffuse or uncertain predictions.53By applying GFL to the binned output, the training process is explicitly guided to improve its predictions for the tail bins corresponding to extreme events, while simultaneously down-weighting the loss from the high-frequency, "easy" bins representing normal behavior. This represents a highly specific, novel, and actionable strategy that perfectly marries a state-of-the-art loss function with the existing model architecture.Direct Gradient Modulation and Adaptive ScalingBeyond modifying the loss function, it is possible to intervene directly at the level of the gradients during backpropagation. These techniques act as dynamic regularizers, ensuring that the large loss values generated by extreme events do not lead to destructively large parameter updates that destabilize training.Adaptive Gradient Clipping (AGC): Standard gradient clipping prevents exploding gradients by capping the norm of the gradient vector at a fixed, user-defined threshold.54 While effective, a single global threshold may be too restrictive for some layers and too permissive for others. Adaptive Gradient Clipping (AGC) improves upon this by making the clipping threshold adaptive.57 For each layer l, the gradient Gl​ is clipped based on the norm of that layer's weight matrix, Wl​. The clipping rule is:$$ \text{if } \frac{|G_l|_F}{|W_l|_F} > \lambda \text{, then } G_l \leftarrow \lambda \frac{|W_l|_F}{|G_l|_F} G_l $$where λ is a clipping factor (e.g., 0.01) and ∥⋅∥F​ is the Frobenius norm. AGC prevents large gradients from extreme event samples from causing disproportionately large updates, especially in layers with small weight norms. This acts as an adaptive learning rate for each layer based on the ratio of gradient norm to weight norm, promoting more stable training in the presence of high-magnitude inputs and errors.54Attention as Implicit Gradient Modulation: While not an explicit technique, the self-attention mechanism within the Transformer architecture can be interpreted as a form of dynamic, input-dependent gradient modulation.58 The attention weights, computed for each input token, directly scale the value vectors before they are aggregated. During backpropagation, these same weights effectively scale the gradients flowing back to the value vectors. By learning to assign high attention scores to specific time steps (e.g., those indicative of an impending extreme event), the model implicitly increases the gradient flow from those steps, thereby focusing the learning process on the most salient information within the input window.60Table 2: Overview of Advanced Loss Functions for Extreme Event RegressionLoss FunctionCore MechanismTarget Data CharacteristicAdaptivityImplementation ComplexityKey ReferencesGEVL-FréchetReplaces the implicit Gaussian kernel of MSE with a heavy-tailed Fréchet kernel from Extreme Value Theory.Very heavy-tailed distributions; focuses on modeling the tail index accurately.Static (fixed functional form).Moderate: Requires implementing a custom loss function based on the Fréchet distribution.4Self-adaptive EPLUses a tripartite penalty function to treat under-prediction, over-prediction, and normal errors asymmetrically.Asymmetric risk profiles; situations where under-prediction of extremes is much costlier than over-prediction.Data-Adaptive: Thresholds and penalty strengths are determined automatically from the data distribution.Moderate: Involves data analysis to set thresholds and a custom piece-wise loss function.6Meta-Weighted LossTreats loss weights as learnable parameters, optimized via bilevel optimization against a validation set of extremes.General data imbalance; learns the optimal weighting scheme for any distribution.Model-Adaptive: Weights are dynamically learned and updated throughout the training process.High: Requires a complex bilevel optimization loop and a separate, clean validation set of extremes.44Generalized Focal Loss (GFL)Generalizes Focal Loss to continuous targets by discretizing the output and applying a focal-style loss to the bin probabilities.High class imbalance where "easy" normal samples dominate the loss over "hard" extreme samples.Static (fixed functional form, but adapts to model confidence pt​).Moderate: Highly compatible with existing binned probabilistic heads; requires implementing QFL and DFL components.51Self-Supervised and Contrastive Learning for Regime DistinctionA more sophisticated approach to improving extreme event forecasting involves enhancing the quality of the model's learned representations. The core hypothesis is that standard training on imbalanced data leads to a feature space where the representations of "normal" states and "pre-extreme" states are entangled and poorly separated. This makes it difficult for the downstream forecasting head to distinguish between these critical regimes. Self-supervised learning (SSL), and particularly contrastive learning, can be employed as an auxiliary training objective to explicitly regularize the model's encoder, forcing it to learn a feature space where these different regimes are more distinct.Rationale for SSL in Extreme Event ForecastingSelf-supervised learning methods create pretext tasks from unlabeled data to learn meaningful representations.62 In the context of extreme events, the primary goal of SSL is not to replace the main forecasting task but to learn a robust representation of "normalcy".64 By pre-training or jointly training the model's encoder on a task that forces it to understand the typical patterns and dynamics of the time series, any deviation from this learned normality—such as the precursors to an extreme event—becomes more salient in the feature space.66 This makes the subsequent task of predicting the magnitude and probability of the extreme event easier for the model's forecasting head.Contrastive Learning as an Auxiliary Training ObjectiveContrastive learning has emerged as a dominant paradigm in SSL.68 It trains an encoder to map inputs to a latent space where representations of "similar" samples are pulled together, while representations of "dissimilar" samples are pushed apart.70 This is typically optimized using a contrastive loss function like InfoNCE, which contrasts a positive pair against a set of negative pairs. When used as an auxiliary objective, the total loss becomes a weighted sum of the primary forecasting loss and the contrastive loss: Ltotal​=LCRPS​+λ⋅Lcontrastive​.72 The contrastive loss is computed on the output of the Transformer/MoE encoder, directly shaping the feature space that the probabilistic head receives.The critical design choice in any contrastive learning framework is the strategy for defining positive and negative pairs. For the specific problem of distinguishing between normal and extreme regimes, standard unsupervised methods are often insufficient. A supervised or pseudo-supervised objective is necessary to learn the required semantic representations.Standard Unsupervised Strategies (Less Effective for Regime Distinction):Temporal Proximity: In this approach, two windows from the same time series that are close in time are considered a positive pair, while windows that are far apart or from different time series are considered negative pairs.73 This teaches the model to learn smooth, temporally consistent representations but fails to distinguish between a normal state and a pre-extreme state that might be temporally adjacent.Augmentation-based: Here, a positive pair consists of a window and a stochastically augmented version of itself (e.g., with added jitter, scaling, or time warping).75 This teaches the model to learn representations that are invariant to these transformations, which can improve robustness to noise. However, it does not explicitly teach the model about the semantic difference between operational regimes.Supervised/Pseudo-labeled Contrastive Learning (Most Effective for Regime Distinction):This is the most promising approach for the user's problem. It leverages the forecasting labels to create semantically meaningful positive and negative pairs that directly teach regime separation.Methodology: First, the training windows are assigned pseudo-labels based on the target values. For example, any window whose future contains a value above the 99th percentile can be labeled "pre-extreme," while all other windows are labeled "normal." The pairs are then constructed based on these labels:Positive Pairs: Two randomly selected windows that both have the "pre-extreme" label.Negative Pairs: A "pre-extreme" window and a "normal" window.Effect: By optimizing a contrastive loss with these pairs, the model is explicitly forced to map all "pre-extreme" states to a tight cluster in the embedding space, far away from the cluster of "normal" states.76 This creates a feature space that is highly structured and separable with respect to the regimes of interest. Recent work like SIP-LDL (Semi-supervised Instance-graph-based Pseudo-Label Distribution Learning) demonstrates that this can be achieved effectively even with a very small number of true labels to guide the process.68This explicit regularization of the feature space is particularly beneficial for the user's Mixture-of-Experts (MoE) architecture. MoE models function by routing different types of inputs to specialized expert sub-networks via a gating mechanism.78 This routing is only effective if the representations fed to the gating network are separable. If the representations for "normal" and "pre-extreme" states are entangled, the gating network cannot learn to route them to different experts. The supervised contrastive auxiliary loss directly addresses this by pre-conditioning the feature space to be "MoE-friendly," creating the separability needed for effective expert specialization.A conceptual implementation of this combined training loop is as follows:Python# Conceptual Pseudo-code for Training with Auxiliary Contrastive Loss
# model: The Transformer/MoE/Pareto model
# crps_loss_fn: The primary CRPS loss function
# contrastive_loss_fn: The supervised contrastive loss (e.g., SupConLoss)
# lambda_contrastive: Weight for the auxiliary loss

for batch in dataloader:
    # batch contains (input_window, target_window, regime_label)
    input_windows, target_windows, regime_labels = batch

    # Forward pass through the encoder part of the model
    # encoder_output shape: (batch_size, seq_len, embedding_dim)
    encoder_output = model.encoder(input_windows)

    # Project encoder output to a lower-dimensional space for contrastive loss
    # projections shape: (batch_size, projection_dim)
    projections = model.projection_head(encoder_output[:, -1, :]) # Use last time step's embedding

    # Compute the supervised contrastive loss on the projections
    # This loss uses regime_labels to define positive/negative pairs
    contrastive_loss = contrastive_loss_fn(projections, regime_labels)

    # Forward pass through the full model to get distribution parameters
    dist_params = model.decoder_and_head(encoder_output)

    # Compute the primary forecasting loss (CRPS)
    crps_loss = crps_loss_fn(dist_params, target_windows)

    # Combine the losses
    total_loss = crps_loss + lambda_contrastive * contrastive_loss

    # Backpropagation and optimizer step
    total_loss.backward()
    optimizer.step()
    optimizer.zero_grad()
Alternative Strategy: Self-Supervised Pre-training and Fine-tuningAn alternative to using SSL as an auxiliary loss is a two-stage approach: pre-training and fine-tuning.Pre-training: The model's encoder (or the entire model) is first trained on a massive, general-purpose, unlabeled corpus of time series data. The training objective is a self-supervised task, such as masked reconstruction (like BERT), forecasting, or contrastive learning.79 This stage allows the model to learn a rich and general understanding of time series dynamics without being biased by a specific, imbalanced dataset.Fine-tuning: The pre-trained model is then fine-tuned on the user's specific, smaller, and imbalanced hydrological dataset using the primary forecasting objective. Research has shown that this transfer learning approach can substantially improve performance on minority classes, as the robust representations learned during pre-training provide a much better starting point for the fine-tuning process.82While powerful, this approach is more resource-intensive, as it requires a large pre-training dataset and significant computational resources. For many applications, using SSL as an auxiliary loss on the target dataset provides a more practical balance of performance and cost.Table 3: Strategies for Positive/Negative Pair Construction in Contrastive Time Series Learning for Regime IdentificationPairing StrategyPositive Pair DefinitionNegative Pair DefinitionWhat the Model LearnsSuitability for Extreme EventsKey ReferencesTemporal ProximityTwo sub-sequences from the same time series that are close in time.Sub-sequences that are far apart in time or from different time series.Temporal smoothness and local consistency of representations.Low. Fails to distinguish between a normal state and an adjacent pre-extreme state. Treats them as similar.73Augmentation InvarianceAn original sub-sequence and a stochastically augmented version of it (e.g., with jitter, scaling).Augmented versions of other sub-sequences in the batch.Invariance to specific transformations (e.g., noise, scaling), leading to more robust features.Moderate. Improves general robustness but does not explicitly teach the semantic difference between regimes.75Supervised Regime LabelingTwo different sub-sequences that share the same pseudo-label (e.g., both are "pre-extreme").A sub-sequence labeled "pre-extreme" and another labeled "normal".Semantic separability of regimes. Forces the model to create distinct, well-separated clusters for normal and pre-extreme states.High. Directly addresses the core problem by regularizing the feature space to be discriminative for extreme event precursors.76Minor Architectural Considerations for Enhanced StabilityWhile the primary focus of this report is on training strategies, certain minor, non-fundamental architectural tweaks can significantly improve the stability and performance of a model when trained on heavy-tailed, non-stationary data. These modifications are not about redesigning the core Transformer/MoE architecture but are rather "plug-and-play" components that help manage the high dynamic range of the data and ensure stable gradient flow, ultimately supporting the probabilistic head in its sensitive task of tail estimation.Specialized Normalization and Activation LayersStandard deep learning components can be brittle when faced with the extreme values and distribution shifts common in hydrological data.Normalization Schemes for Non-Stationary DataIn Transformers, Layer Normalization is the standard choice. However, for time series with extreme events, its global nature (normalizing across all features for a given time step) can be problematic. The statistics (mean and variance) used for normalization can be heavily skewed by a single extreme value, potentially washing out important information in other, lower-magnitude channels. A more sophisticated approach to normalization in this context is not just about standardization; it is about disentangling the magnitude of an event from its underlying temporal pattern.Instance Normalization: Originally developed for style transfer in images, Instance Normalization normalizes each sample (and each channel within that sample) independently. In the time series context, this means normalizing each training window independently of others in the batch.84 This has the effect of removing instance-specific "style," which can be interpreted as the scale or magnitude of the values in that window. By doing so, it allows the network to learn temporal patterns that are scale-invariant. This is critical for extreme events, as it enables the model to recognize a "pre-flood" pattern regardless of whether it occurs in a small tributary (low absolute magnitude) or a major river (high absolute magnitude), improving generalization.Adaptive Normalization Frameworks (Morphing-Flow): Recent research has proposed more advanced, learnable normalization layers. The Morphing-Flow (MoF) framework, for example, combines a spline-based transformation layer with a test-time adaptation method.85 This module adaptively normalizes the non-stationary, fat-tailed input distribution into a more well-behaved, normal-like distribution while explicitly preserving critical extreme features. By ensuring that the inputs to the network's main layers remain within an effective activation space, even under severe distributional drift, such frameworks can dramatically improve model stability and convergence.85Adaptive Activation FunctionsThe choice of activation function can also impact training stability and performance. The standard Rectified Linear Unit (ReLU) is simple and effective but can suffer from the "dying ReLU" problem, where neurons become permanently inactive if their input is always negative. This can be exacerbated by the sparse and highly variable activation patterns that result from data with extreme values.Swish/SiLU: Modern alternatives like Swish (also known as SiLU, Sigmoid-weighted Linear Unit), defined as f(x)=x⋅σ(x), where σ is the sigmoid function, offer several advantages.86 Swish is a smooth, non-monotonic function that is unbounded above (like ReLU) but bounded below. Its smoothness helps with gradient flow and optimization stability. Its non-monotonic "bump" around zero can help it escape local minima and learn more complex functions. For time series problems, Swish has been found to consistently outperform other activation functions, likely due to its ability to handle the "dying ReLU" problem and its continuous derivative at zero.86Adaptive Activations: Further research is exploring activation functions with learnable parameters, allowing them to adapt their shape to the specific data distribution of a given task or even a specific layer within the network.87Supporting the Probabilistic HeadThese seemingly minor architectural tweaks have a direct and significant impact on the performance of the final probabilistic output head. The Spliced Binned Pareto distribution is a complex, multi-parameter distribution. The estimation of its parameters, especially the tail index parameter that governs the heaviness of the Pareto tail, is highly sensitive to the quality of the input representations it receives from the encoder.10If the internal activations of the network are unstable, saturated, or poorly scaled due to extreme input values, the representations passed to the final layer will be noisy and unreliable. This can lead to inaccurate and unstable estimates of the distribution's parameters, resulting in poor probabilistic forecasts. By employing specialized normalization layers to disentangle magnitude from pattern and adaptive activation functions to ensure smooth gradient flow, the model can produce more stable, well-scaled, and informative representations. These high-quality representations provide a better foundation for the probabilistic head, enabling it to more accurately and robustly estimate the parameters of the heavy-tailed distribution, particularly in the critical tail regions.Conclusion and Strategic RecommendationsThe challenge of accurately forecasting rare, high-impact hydrological events is not an intractable problem of model capacity but a solvable challenge of training methodology. A sophisticated architecture, such as the Transformer/MoE model with a Spliced Binned Pareto head described, possesses the necessary components to capture complex temporal dependencies and model heavy-tailed distributions. However, to unlock its full potential, it must be paired with a training strategy that explicitly counteracts the inherent data imbalance and forces the model to learn from the very events it would otherwise ignore. This review has surveyed the state-of-the-art in such training strategies, spanning advanced data sampling, dynamic loss and gradient modulation, and auxiliary self-supervised objectives.The key findings indicate a clear trend away from static, heuristic-based methods towards dynamic, adaptive, and principled approaches. In data sampling, the most effective techniques define sample "hardness" not as an intrinsic property of the data, but as a dynamic measure of the model's learning progress, using metrics like Dynamic Instance Hardness. In loss modification, the frontier has moved beyond simple re-weighting to fundamentally re-parameterizing the loss function based on Extreme Value Theory (e.g., GEVL) or adapting concepts like Focal Loss to the binned probabilistic output space (e.g., GFL). Finally, contrastive learning, when used as a supervised auxiliary objective, offers a powerful mechanism to regularize the model's feature space, creating explicit separability between normal and pre-extreme states, which is crucial for components like Mixture-of-Experts layers.Based on this analysis, a tiered implementation roadmap is proposed, allowing for a progressive enhancement of the training pipeline by balancing implementation complexity with potential performance impact.A Tiered Implementation RoadmapTier 1: High-Impact, Lower Complexity FoundationImplement Importance-Weighted Sampling: The most immediate and impactful change is to move from uniform to weighted sampling.Action: Utilize torch.utils.data.WeightedRandomSampler.Hardness Score: Begin with a robust, hybrid hardness score. A highly effective and practical choice is to combine a model-based score with a data-driven one. Pre-train a simple autoencoder on all "normal" (non-extreme) data and use the reconstruction error for each training window as a base score. Multiply this score by the normalized magnitude of the target value within the window to further emphasize the most severe events. This combines structural anomaly detection with a direct focus on impact.Tier 2: SOTA Performance through Objective and Gradient RefinementAdapt the Loss Function: Augment or replace the standard CRPS loss with an objective more attuned to extremes.Action (Recommended): Implement Generalized Focal Loss (GFL). Given the model's existing binned output head, this is a natural and powerful fit. Apply the QFL and DFL components to the logits of the Spliced Binned Pareto bins. This will directly focus the model on improving its categorical predictions for the tail bins without requiring a fundamental change to the probabilistic output.Action (Alternative): If the primary goal is to more rigorously model the statistical properties of the tail, implement a Generalized Extreme Value Loss (GEVL), specifically one based on the Fréchet distribution, which is well-suited for heavy-tailed hydrological data.Implement Adaptive Gradient Clipping (AGC): To ensure training stability with the more aggressive loss functions and sampling strategies, replace standard gradient clipping.Action: Integrate AGC, which scales the clipping threshold for each layer based on its weight norm. This will prevent large gradients from extreme event samples from destabilizing the training process.Tier 3: Research-Intensive, Maximum PotentialIntegrate a Supervised Contrastive Auxiliary Loss: This represents the most advanced step, aiming to fundamentally restructure the model's learned feature space for better regime separation.Action:Create pseudo-labels for the training data by identifying windows that precede an extreme event (e.g., within 24 hours of a flood peak) as the "pre-extreme" class.Add a projection head to the output of the Transformer/MoE encoder.Implement a supervised contrastive loss (e.g., SupCon loss) on the outputs of this projection head, using the pseudo-labels to define positive ("pre-extreme" vs. "pre-extreme") and negative ("pre-extreme" vs. "normal") pairs.Add this contrastive loss to the total loss function with a tunable weight, λ. This will explicitly train the encoder to produce separable representations, which is highly beneficial for the MoE gating mechanism.These strategies are not mutually exclusive. An optimal pipeline would likely combine elements from all three tiers: using weighted sampling from Tier 1 to construct batches, optimizing with a GFL-enhanced loss and AGC from Tier 2, and regularizing the encoder's representation space with the contrastive auxiliary loss from Tier 3. By adopting such a holistic and adaptive training framework, it is possible to transform a model that is merely accurate on average into one that is truly robust and reliable in predicting the rare, high-impact events that matter most.