Modeling Semicontinuous Phenomena: A Comprehensive Report on the Theory and Application of Zero-Inflated Continuous DistributionsSection 1: The Conceptual Framework of Zero-Inflated Continuous Models1.1. The Challenge of Semicontinuous DataIn numerous scientific and economic disciplines, researchers encounter data that are neither purely continuous nor purely discrete. These data, often termed semicontinuous or zero-inflated continuous, are characterized by a significant accumulation of observations at a single point, typically zero, combined with a continuous distribution of positive values.1 Such data structures are prevalent in fields like hydrology, economics, ecology, and biomedical research. For instance, daily precipitation records frequently contain a large proportion of "dry days" with zero rainfall, while the rainy days exhibit a continuous, positively skewed distribution of rainfall amounts.2 Similarly, in healthcare economics, individual medical expenditures often include a substantial number of subjects with zero costs (those who did not use services) alongside a right-skewed distribution of positive costs for those who did.1 Other canonical examples include data on substance use, where zero represents abstinence, and ecological data where zero may indicate the absence of a species or biomass at a given site.1The primary analytical challenge posed by semicontinuous data is the inadequacy of standard statistical models. Classical continuous distributions, such as the Normal, Gamma, or Lognormal, are defined by a probability density function (PDF) that integrates to one over their support. For any continuous random variable, the probability of observing any single exact value is, by definition, zero. Therefore, a model based solely on a continuous PDF cannot account for the point mass, or "spike," at zero that is empirically observed in the data.1 Attempting to fit a standard continuous distribution to semicontinuous data invariably leads to poor model fit, biased parameter estimates, and incorrect inferences about the underlying data-generating process.8 Conversely, models designed for count data, like the Poisson or Negative Binomial distributions, are inappropriate because the positive values are not restricted to integers.9 This unique mixture of a discrete point mass and a continuous distribution necessitates a specialized modeling framework.1.2. The Two-Part Model: A Foundational ClarificationThe terminology used to describe models for semicontinuous data can be a source of confusion, largely stemming from the historical development of related models for count data. The term "zero-inflated" was popularized by Lambert's work on the Zero-Inflated Poisson (ZIP) model, which addresses count data with an excess of zeros relative to a standard Poisson process.4 A key assumption of the ZIP model is that zeros can arise from two distinct sources: a "structural" process that always produces zeros (e.g., a subject is not at risk of the event) and a "sampling" process where the count-generating mechanism itself produces a zero by chance (e.g., a subject is at risk, but no events were observed during the study period).10This conceptualization, however, does not directly translate to continuous data. As previously noted, a continuous distribution f(y) defined for y > 0 has a probability of exactly zero at y=0.7 Consequently, there can be no "sampling zeros" arising from the continuous component of the model. Any and all observed zero values must originate from a separate, discrete process that generates the point mass at zero. This fundamental distinction means that models for semicontinuous data are not "zero-inflated" in the same sense as their count data counterparts.Instead, the more accurate and mechanistically descriptive term for these models is the two-part model (2PM) or, equivalently, the hurdle model.1 This framework explicitly conceptualizes the data-generating process in two stages. First, a binary process determines whether an observation is zero or positive—whether the "hurdle" at zero is crossed. Second, conditional on the observation being positive, a continuous distribution models the magnitude of that positive value.1 While the term "zero-inflated continuous model" is sometimes used in the literature, particularly in software documentation, this report will primarily use the more precise "two-part model" terminology to maintain conceptual and mathematical clarity.13 This distinction is not merely semantic; it is critical for correctly formulating the model's likelihood and interpreting its parameters.1.3. Mathematical Formulation of the Two-Part ModelThe two-part model for a semicontinuous random variable Y is formally defined as a finite mixture model. It combines a degenerate distribution (a point mass) at zero with a conditional continuous distribution for the positive values.5 The probability density function (PDF), h(y), of this mixed distribution can be written as:h(y∣π,θ)={π(1−π)f(y∣θ)​if y=0if y>0​This can be expressed more compactly using an indicator function I(y=0):h(y∣π,θ)=πI(y=0)[(1−π)f(y∣θ)]1−I(y=0)Here, the components are:y: The observed value of the semicontinuous random variable.\pi: The probability of a zero observation, P(Y=0). This is the parameter governing the binary (hurdle) component.1-\pi: The probability of a positive observation, P(Y>0).f(y | \boldsymbol{\theta}): The probability density function for the continuous part of the model, conditional on Y > 0. This function is defined only for y > 0. In some formulations, this is written as f(y | \boldsymbol{\theta}, y>0) to make the conditioning explicit.\boldsymbol{\theta}: A vector of parameters (e.g., location, scale, shape) for the continuous distribution f(y).The likelihood function for a set of n independent observations y_1, y_2,..., y_n is constructed by considering the contributions from the zero and positive values separately. Let the set of indices for zero observations be I_0 = \{i | y_i = 0\} and for positive observations be I_+ = \{i | y_i > 0\}. The total likelihood L is the product of the probabilities for each observation:L(π,θ∣y)=(i∈I0​∏​πi​)​i∈I+​∏​(1−πi​)f(yi​∣θi​)​The log-likelihood function, which is typically maximized for parameter estimation, is therefore:logL(π,θ∣y)=i∈I0​∑​log(πi​)+i∈I+​∑​log(1−πi​)+i∈I+​∑​log(f(yi​∣θi​))A crucial property of this log-likelihood function is its decomposability. The expression can be split into two separate parts that do not share parameters:A part for the binary process (zero vs. positive): \sum_{i \in I_0} \log(\pi_i) + \sum_{i \in I_+} \log(1-\pi_i)A part for the continuous process (magnitude of positives): \sum_{i \in I_+} \log(f(y_i | \boldsymbol{\theta}_i))This separability implies that the parameters governing the probability of a zero outcome (\pi) can be estimated independently from the parameters governing the distribution of the positive outcomes (\boldsymbol{\theta}). This greatly simplifies the estimation procedure, as two separate, and often simpler, models can be fitted instead of a single complex one.11.4. Distinguishing from Related ModelsTo fully situate the two-part model, it is essential to distinguish it from other models that handle non-standard data distributions.Hurdle vs. True Zero-Inflated Models: As established, the two-part model for continuous data is functionally a hurdle model. In the context of count data, a hurdle model uses a zero-truncated distribution (e.g., truncated Poisson) for the positive counts, enforcing the idea that once the zero hurdle is passed, the outcome must be strictly positive (1, 2, 3,...).11 A true zero-inflated model, by contrast, uses a standard, non-truncated count distribution (e.g., Poisson) for the second part, allowing for zeros to be generated by both the binary "inflation" process and the count process itself.11 For continuous data, since the continuous part cannot generate zeros, the distinction becomes moot, and the hurdle/two-part structure is the only viable one.13Two-Part vs. Tobit Models: The Tobit model, developed by James Tobin, is another approach for data with a cluster of values at a limit, but its underlying assumption is fundamentally different.1 The Tobit model is designed for censored data. It assumes that there is an underlying latent variable that is continuous and normally distributed, but it is only observed if it exceeds a certain threshold (the censoring point, often zero). Observations at the threshold are not considered "true" zeros but are instead treated as censored values, where the true value is known only to be less than or equal to the threshold.1The choice between a two-part model and a Tobit model depends entirely on the data-generating process. For medical cost data, if a zero represents a patient who truly did not use any medical services, it is a "true zero," and a two-part model is appropriate. If, however, the data represents measurements from a device with a lower limit of detection, and all values below this limit are recorded as zero, then a Tobit model is more plausible.1 In the case of daily precipitation, a value of zero unequivocally means "no rain occurred." It is a true absence of the phenomenon, not a censored measurement. Therefore, the two-part model is the conceptually correct framework for analyzing such data.1Section 2: Parametrizing the Model ComponentsA primary strength of the two-part modeling framework is its ability to incorporate covariates to explain variability in both the probability of a zero outcome and the distribution of the positive values. This allows for a nuanced investigation of the factors driving the occurrence versus the magnitude of a phenomenon.2.1. Modeling the Zero-Probability Component (The Hurdle)In most real-world applications, the probability of observing a zero, \pi, is not a fixed constant across all observations. It is expected to vary depending on a set of explanatory variables. The standard approach for modeling this relationship is to use a generalized linear model (GLM) for a binary outcome.The most common choice is logistic regression, which models the log-odds of the event (in this case, observing a zero) as a linear function of the covariates.9 Let \mathbf{w}_i be the vector of covariates for the i-th observation that are hypothesized to influence the probability of a zero outcome. The model is specified via the logit link function:logit(πi​)=log(1−πi​πi​​)=wiT​αwhere \boldsymbol{\alpha} is the vector of regression coefficients to be estimated. The probability \pi_i can then be recovered as:πi​=1+exp(wiT​α)exp(wiT​α)​The coefficients in \boldsymbol{\alpha} are interpreted as log-odds ratios. For example, a one-unit increase in a continuous covariate w_j is associated with a change in the log-odds of a zero outcome by \alpha_j. Exponentiating the coefficient, exp(\alpha_j), gives the odds ratio. An alternative to the logit link is the probit link, which assumes an underlying standard normal distribution and models the probability using the normal cumulative distribution function (CDF).9 While both are often used, the logistic model is generally preferred for its simpler interpretation in terms of odds ratios.2.2. Modeling the Continuous Component (The Positive Values)Similarly, the parameters of the continuous distribution for the positive values, f(y | \boldsymbol{\theta}), are also modeled as functions of covariates. Let \mathbf{x}_i be the vector of covariates for the i-th observation that are thought to influence the magnitude of the positive outcomes.A highly powerful and flexible framework for this task is the Generalized Additive Models for Location, Scale, and Shape (GAMLSS), developed by Rigby and Stasinopoulos.20 GAMLSS extends the traditional GLM framework by allowing not just the location parameter (e.g., the mean, \mu) but all parameters of the conditional distribution of the response variable to be modeled as functions of covariates. This can include parameters for scale (e.g., standard deviation, \sigma) and shape (e.g., skewness \nu, kurtosis \tau).20For each parameter \theta_k in the parameter vector \boldsymbol{\theta} = (\theta_1, \theta_2,..., \theta_p), a GAMLSS model specifies a relationship of the form:gk​(θki​)=xkiT​βk​+j=1∑Jk​​skj​(zkji​)where:g_k(.) is a known link function that ensures \theta_{ki} remains within its valid range. For parameters that must be positive, such as the mean \mu or scale \sigma in a Gamma or Lognormal distribution, a log link is typically used: g_k(\theta_{ki}) = \log(\theta_{ki}).9\mathbf{x}_{ki}^T \boldsymbol{\beta}_k represents the parametric linear part of the model for parameter \theta_k.s_{kj}(.) are non-parametric smooth functions (e.g., splines) of covariates z_{kji}, allowing for flexible, non-linear relationships to be captured.This framework allows for the explicit modeling of, for example, heteroscedasticity by allowing the scale parameter \sigma to depend on covariates.The separation of the two model parts provides immense flexibility in specifying the covariates. The set of predictors \mathbf{w} for the zero-probability component can be entirely different from the set of predictors \mathbf{x} for the continuous component.16 Even when the same variables are used, their estimated effects (\boldsymbol{\alpha} and \boldsymbol{\beta}) can differ, sometimes even pointing in opposite directions. This feature is not just a matter of statistical convenience; it allows for the construction of more realistic and mechanistically interpretable models.Consider the application to daily precipitation. The physical processes that determine whether a day is dry or wet (the hurdle component) may differ from those that determine how much rain falls on a wet day (the continuous component).The Hurdle (Dry vs. Wet): The probability of a dry day, \pi, might be driven by large-scale atmospheric patterns, such as the position of high-pressure systems, synoptic-scale atmospheric moisture content, or seasonal cycles. A model for \pi could therefore include covariates like sea-level pressure, a seasonal term (e.g., sine and cosine of the day of the year), and large-scale climate indices like the El Niño-Southern Oscillation (ENSO) index.The Continuous Part (Rainfall Intensity): Conditional on a day being wet, the intensity of the rainfall might be governed by more localized or thermodynamic factors. These could include variables like convective available potential energy (CAPE), which fuels thunderstorms, local temperature, which influences the atmosphere's water-holding capacity, and orographic effects related to elevation.A modeling framework that forces the same predictors and effects for both processes would obscure these distinct physical drivers. The two-part model, by allowing for different covariate sets \mathbf{w} and \mathbf{x}, enables researchers to test separate and more refined hypotheses about the factors governing the occurrence versus the intensity of precipitation, leading to a deeper scientific understanding.